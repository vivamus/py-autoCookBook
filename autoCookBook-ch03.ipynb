{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. First Web Scrapping\n",
    "### Downloading web pages (p. 72)\n",
    "`$ echo \"requests==2.23.0\" >> requirements.txt` -- had to change to 2.22.0 <br>\n",
    "Will download __[this columbia sample wep page](http://www.columbia.edu/~fdc/sample.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.columbia.edu/~fdc/sample.html'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.columbia.edu/~fdc/sample.html'\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "response.text\n",
    "response.headers\n",
    "response.request.headers\n",
    "response.request\n",
    "response.request.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[request module docs](https://requests.readthedocs.io/en/master/)__ <br>\n",
    "__[status codes](https://httpstatuses.com/)__ They are also described in the `http.HTTPStatus` enum with convenient constant names, such as OK, NOT_FOUND, or FORBIDDEN\n",
    "\n",
    "`$ echo \"beautifulsoup4==4.8.2\" >> requirements.txt`   __[Beautiful Soup doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\usr\\yuri\\python\\py-autoCookBook\\autoCookBook-ch03.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/usr/yuri/python/py-autoCookBook/autoCookBook-ch03.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/usr/yuri/python/py-autoCookBook/autoCookBook-ch03.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/usr/yuri/python/py-autoCookBook/autoCookBook-ch03.ipynb#ch0000003?line=2'>3</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttp://www.columbia.edu/~fdc/sample.html\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/usr/yuri/python/py-autoCookBook/autoCookBook-ch03.ipynb#ch0000003?line=3'>4</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://www.columbia.edu/~fdc/sample.html'\n",
    "response = requests.get(url)\n",
    "\n",
    "page = BeautifulSoup(response.text, 'html.parser')\n",
    "page.title\n",
    "page.title.string\n",
    "page.find_all('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the text on the section for Special Characters. Stop when you reach the next `<h3>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_section = page.find('h3', attrs={'id':'chars'}) # tag <a>\n",
    "section = []\n",
    "for el in link_section.next_elements:\n",
    "    if el.name == 'h3':\n",
    "        break\n",
    "    section.append(el.string or '') # None if el has no text\n",
    "\n",
    "result = ''.join(section)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "page.find_all( re.compile('(h2|h3)'))  #regex in find_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the web (p. 79)\n",
    "\n",
    "Download the whole __[test_site directory](https://github.com/PacktPublishing/Python-Automation-Cookbook-Second-Edition/tree/master/Chapter03/test_site)__  using DownGit<br>\n",
    "Start server below with bang.\n",
    "Check browser at __[http://localhost:8000](http://localhost:8000)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server, use <Ctrl-C> to stop\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:29] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:30] \"GET /files/b93bec5d9681df87e6e8d5703ed7cd81-2.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:30] \"GET /files/5eabef23f63024c20389c34b94dee593-1.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:31] \"GET /files/33714fc865e02aeda2dabb9a42a787b2-0.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:31] \"GET /files/archive-september-2018.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:32] \"GET /index.html HTTP/1.1\" 200 -\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuri/usr/python/py-autoCookBook/test_site/simple_delay_server.py\", line 25, in <module>\n",
      "    server.serve_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 232, in serve_forever\n",
      "    ready = selector.select(poll_interval)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd test_site; python simple_delay_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From same site download `ch03-crawl_web.py` and search for references to `python` or `crocodile` <br>\n",
    "But better previous server or this call, since Jupyter executes only one bang at a time\n",
    "```\n",
    "$ python ch03-crawl_web.py http://localhost:8000/ -p python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ch03-crawl_web.py http://localhost:8000/ -p python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Components of crawl_web.py\n",
    "1. A loop that goes through all the found links, in the `main` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(base_url, to_search):\n",
    "    checked_links = set()\n",
    "    to_check = [base_url]\n",
    "    max_checks = 10\n",
    "\n",
    "    while to_check and max_checks:\n",
    "        link = to_check.pop()\n",
    "        links = process_link(link, text=to_search)\n",
    "        checked_links.add(link)\n",
    "        for link in links:\n",
    "            if link in checked_links:\n",
    "                continue\n",
    "            checked_links.add(link)\n",
    "            to_check.append(link)\n",
    "        max_check -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. download and parse links in `parse_link`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "import http\n",
    "def process_link(source_link, pat):\n",
    "    logging.info(f'exctracting links from {source_link}')\n",
    "    result = requests.get()\n",
    "    if result.status_code != http.client.ok:\n",
    "        logging.error(f'Failed retrieve {source_link}: {result}')\n",
    "        return []\n",
    "    if 'html' not in result.headers['Content-type']: #skip PDF\n",
    "        logging.info(f'Not HTML: {source_link}')\n",
    "        return []\n",
    "    page = BeautifulSoup(result.text,'html.parser')\n",
    "    search_text(source_link,page,pat)\n",
    "    parsed_source = urlparse(source_link) # divides URL to elements: http site path etc\n",
    "    return get_links(parsed_source,page)\n",
    "\n",
    "def search_text(source_link, page, pat):\n",
    "    '''print elements with text pattern'''\n",
    "    for el in page.find_all(text = re.compile(pat,flags=re.IGNORECASE) ) :\n",
    "        print(f'Link {source_link} ==> {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The `get_links` function retrieves all links on a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "def get_links(parsed_source,page):\n",
    "    '''retieve links on the page'''\n",
    "    links = []\n",
    "    for el in page.find_all('a'): # <a> elements\n",
    "        link = el.get('href')\n",
    "        if not link: continue\n",
    "        if link.startwith('#'): continue  # inside page\n",
    "        if link.startwith('mailto:'): continue  \n",
    "        if not link.startwith('http'):    # local link\n",
    "            netloc = parsed_source.netloc\n",
    "            scheme = parsed_source.scheme\n",
    "            path = urljoin(parsed_source.path, link)\n",
    "            link = f'{scheme}://{netloc}{path}'\n",
    "        if parsed_source.netloc not in link: # accept only same domain\n",
    "            continue\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subscribing to feeds\n",
    "`$ echo \"feedparser==5.2.1\" >> requirements.txt`  `use_2to3` is invalid\n",
    "Since this __[dirty fix](https://pypi.org/project/feedparser/5.2.1/)__ is ugly, I'll skip this section for RSS\n",
    "\n",
    "### Accessing web APIs\n",
    "RESTful API using __[JSON](https://www.json.org/)__  -- `requests` has native support<br>\n",
    "__[RESTful](https://codewords.recurse.com/issues/five/what-restful-actually-means)__ uses GET POST DELETE etc\n",
    "\n",
    "We will use __[https://jsonplaceholder.typicode.com](https://jsonplaceholder.typicode.com)__ -- It simulates a common case with posts, comments, and other common resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 10,\n",
       " 'id': 100,\n",
       " 'title': 'at nam consequatur ea labore ea harum',\n",
       " 'body': 'cupiditate quo est a modi nesciunt soluta\\nipsa voluptas error itaque dicta in\\nautem qui minus magnam et distinctio eum\\naccusamus ratione error aut'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "URL = 'https://jsonplaceholder.typicode.com'\n",
    "result = requests.get(URL+'/posts')  # <Response [200]>\n",
    "result.json() # 100 posts\n",
    "result.json()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://jsonplaceholder.typicode.com/posts/101'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = {'userId' : 10, 'title' : 'a title', 'body' : 'some stuff'}\n",
    "result = requests.post(URL+'/posts', json=new_post)\n",
    "result        # <Response [201]>\n",
    "result.json() #{'userId': 10, 'title': 'a title', 'body': 'some stuff', 'id': 101}\n",
    "result.headers['Location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch an existing post with GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(URL+'/posts/2')  # <Response [200]>\n",
    "result\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PATCH to update its values. Check the returned resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 1, 'id': 2, 'title': 'qui est esse', 'body': 'new body'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update = {'body' : 'new body'}\n",
    "result = requests.patch(URL+'/posts/2', json=update) \n",
    "result\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with forms\n",
    "__[test site](https://httpbin.org/forms/post)__ renders the form, but internally calls __[the URL](https://httpbin.org/post)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments', 'custemail', 'custname', 'custtel', 'delivery', 'size', 'topping'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "response = requests.get('https://httpbin.org/forms/post') # forms/post fpr get\n",
    "page = BeautifulSoup(response.text)\n",
    "form = page.find('form')\n",
    "{ field.get('name')    for field in form.find_all( re.compile('input|textarea') )   }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare data to post as a dictionary, post\n",
    "(in API we used Content-Type as `application/json` now `application/x-www-form-urlencoded` -- 400 if incorrect.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {'comments': '',\n",
       "  'custemail': 'sean@oconnell.ie',\n",
       "  'custname': \"Sean O'Connell\",\n",
       "  'custtel': '123-456-789',\n",
       "  'delivery': '20:30',\n",
       "  'size': 'small',\n",
       "  'topping': ['bacon', 'onion']},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate',\n",
       "  'Content-Length': '140',\n",
       "  'Content-Type': 'application/x-www-form-urlencoded',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.22.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-62df0cba-6461d54e52fd48ab256a1a73'},\n",
       " 'json': None,\n",
       " 'origin': '149.117.75.11',\n",
       " 'url': 'https://httpbin.org/post'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'custname': \"Sean O'Connell\", \n",
    "        'custtel': '123-456-789', \n",
    "        'custemail': 'sean@oconnell.ie', \n",
    "        'size': 'small', \n",
    "        'topping': ['bacon', 'onion'], \n",
    "        'delivery': '20:30', \n",
    "        'comments': ''}\n",
    "response = requests.post('https://httpbin.org/post', data)  # /post to post\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Cross-Site Request Forgery (CSRF)](https://stackoverflow.com/a/33829607)__ -- first download the form, as shown in the\n",
    "recipe, obtain the value of the CSRF token, and resubmit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form.find( attrs={'name' : 'token'}).get('value')\n",
    "form.find( attrs={'name' : 'token'})  # did not work, None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium for advanced interaction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3fb195f1769707917c36734594a5b5cdfc69d3756177959b80c085aef1025cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
