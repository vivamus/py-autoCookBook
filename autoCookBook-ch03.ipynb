{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. First Web Scrapping\n",
    "### Downloading web pages (p. 72)\n",
    "`$ echo \"requests==2.23.0\" >> requirements.txt` -- had to change to 2.22.0 <br>\n",
    "Will download __[this columbia sample wep page](http://www.columbia.edu/~fdc/sample.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.columbia.edu/~fdc/sample.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://www.columbia.edu/~fdc/sample.html'\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "response.text\n",
    "response.headers\n",
    "response.request.headers\n",
    "response.request\n",
    "response.request.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[request module docs](https://requests.readthedocs.io/en/master/)__ <br>\n",
    "__[status codes](https://httpstatuses.com/)__ They are also described in the `http.HTTPStatus` enum with convenient constant names, such as OK, NOT_FOUND, or FORBIDDEN\n",
    "\n",
    "`$ echo \"beautifulsoup4==4.8.2\" >> requirements.txt`   __[Beautiful Soup doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 id=\"contents\">CONTENTS</h3>,\n",
       " <h3 id=\"basics\">1. Creating a Web Page</h3>,\n",
       " <h3 id=\"syntax\">2. HTML Syntax</h3>,\n",
       " <h3 id=\"chars\">3. Special Characters</h3>,\n",
       " <h3 id=\"convert\">4. Converting Plain Text to HTML</h3>,\n",
       " <h3 id=\"effects\">5. Effects</h3>,\n",
       " <h3 id=\"lists\">6. Lists</h3>,\n",
       " <h3 id=\"links\">7. Links</h3>,\n",
       " <h3 id=\"tables\">8. Tables</h3>,\n",
       " <h3 id=\"viewing\">9. Viewing Your Web Page</h3>,\n",
       " <h3 id=\"install\">10. Installing Your Web Page on the Internet</h3>,\n",
       " <h3 id=\"more\">11. Where to go from here</h3>,\n",
       " <h3 id=\"fluid\">12. Postscript: Cell Phones</h3>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://www.columbia.edu/~fdc/sample.html'\n",
    "response = requests.get(url)\n",
    "\n",
    "page = BeautifulSoup(response.text, 'html.parser')\n",
    "page.title\n",
    "page.title.string\n",
    "page.find_all('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the text on the section for Special Characters. Stop when you reach the next `<h3>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. Special Characters\\n\\nHTML special \"character entities\" start with ampersand (&&) and\\nend with semicolon (;;), like \"&euro;&euro;\" = \"€\".  The\\never-popular \"no-break space\" is &nbsp;&nbsp;.  There are special\\nentity names for accented Latin letters and other West European special\\ncharacters such as:\\n\\n\\n\\n\\n\\n\\n&auml;&auml;\\na-umlaut\\n\\xa0ä\\xa0\\n\\n\\n&Auml;&Auml;\\nA-umlaut \\n\\xa0Ä\\xa0\\n\\n\\n&aacute;&aacute;\\na-acute \\n\\xa0á\\xa0\\n\\n\\n&agrave;&agrave;\\na-grave \\n\\xa0à\\xa0\\n\\n\\n&ntilde;&ntilde;\\nn-tilde \\n\\xa0ñ\\xa0\\n\\n\\n&szlig;&szlig;\\nGerman double-s\\n\\xa0ß\\xa0\\n\\n\\n&thorn;&thorn;\\nIcelandic thorn \\n\\xa0þ\\xa0\\n\\xa0þ\\xa0\\n\\n\\n\\n\\n\\n(The table above is shown in the basic, default style of HTML.  Of course\\nthere are many ways to customize the appearance of tables; more\\nabout this belowbelow.\\n\\n\\n\\nExamples:\\n\\n\\nFor SpanishSpanish you would need:\\n&Aacute;&Aacute; (Á),\\n&aacute;&aacute; (á),\\n&Eacute;&Eacute; (É),\\n&eacute;&eacute; (é),\\n&Iacute;&Iacute; (Í),\\n&iacute;&iacute; (í),\\n&Oacute;&Oacute; (Ó),\\n&oacute;&oacute; (ó),\\n&Uacute;&Uacute; (ú),\\n&uacute;&uacute; (ú),\\n&Uuml;&Uuml; (Ü),\\n&uuml;&uuml; (ü),\\n&Ntilde;&Ntilde; (Ñ),\\n&ntilde;&ntilde; (ñ);\\n&iquest;&iquest; (¿);\\n&iexcl;&iexcl; (¡).\\nExample: Añorarán = A&ntilde;orar&aacute;nA&ntilde;orar&aacute;n.\\n\\n\\nFor GermanGerman you would need:\\n&Auml;&Auml; (Ä),\\n&auml;&auml; (ä),\\n&Ouml;&Ouml; (Ö),\\n&ouml;&ouml; (ö),\\n&Uuml;&Uuml; (ü),\\n&uuml;&uuml; (ü),\\n&szlig;&szlig; (ß).\\nExample: Grüße aus Köln = Gr&uuml;&szlig;e aus K&ouml;lnGr&uuml;&szlig;e aus K&ouml;ln.\\n\\n\\n\\nCLICK HERECLICK HERE\\nfor a complete list.  When the page encoding is\\nUTF-8UTF-8, which is\\nrecommended, you can also enter any character at all, Roman,\\nCyrillic, Arabic, Hebrew, Greek. Japanese,\\netc, either as numeric entities or (if you have a way to type them) directly\\nfrom the keyboard.\\n\\n\\n\\nAnd remember: if you want to\\ninclude <<, &&,\\nor >> literally in text to be displayed, you have\\nto write &lt;&lt;,\\n&amp;&amp;, &gt;&gt;, respectively.\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_section = page.find('h3', attrs={'id':'chars'}) # tag <a>\n",
    "section = []\n",
    "for el in link_section.next_elements:\n",
    "    if el.name == 'h3':\n",
    "        break\n",
    "    section.append(el.string or '') # None if el has no text\n",
    "\n",
    "result = ''.join(section)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>Do-It-Yourself Web Authoring - a beginner's HTML tutorial</h2>,\n",
       " <h3 id=\"contents\">CONTENTS</h3>,\n",
       " <h3 id=\"basics\">1. Creating a Web Page</h3>,\n",
       " <h3 id=\"syntax\">2. HTML Syntax</h3>,\n",
       " <h3 id=\"chars\">3. Special Characters</h3>,\n",
       " <h3 id=\"convert\">4. Converting Plain Text to HTML</h3>,\n",
       " <h3 id=\"effects\">5. Effects</h3>,\n",
       " <h3 id=\"lists\">6. Lists</h3>,\n",
       " <h3 id=\"links\">7. Links</h3>,\n",
       " <h3 id=\"tables\">8. Tables</h3>,\n",
       " <h3 id=\"viewing\">9. Viewing Your Web Page</h3>,\n",
       " <h3 id=\"install\">10. Installing Your Web Page on the Internet</h3>,\n",
       " <h3 id=\"more\">11. Where to go from here</h3>,\n",
       " <h3 id=\"fluid\">12. Postscript: Cell Phones</h3>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "page.find_all( re.compile('(h2|h3)'))  #regex in find_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the web (p. 79)\n",
    "\n",
    "Download the whole __[test_site directory](https://github.com/PacktPublishing/Python-Automation-Cookbook-Second-Edition/tree/master/Chapter03/test_site)__  using DownGit<br>\n",
    "Start server below with bang.\n",
    "Check browser at __[http://localhost:8000](http://localhost:8000)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server, use <Ctrl-C> to stop\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:29] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:30] \"GET /files/b93bec5d9681df87e6e8d5703ed7cd81-2.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:30] \"GET /files/5eabef23f63024c20389c34b94dee593-1.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:31] \"GET /files/33714fc865e02aeda2dabb9a42a787b2-0.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:31] \"GET /files/archive-september-2018.html HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jul/2022 15:00:32] \"GET /index.html HTTP/1.1\" 200 -\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuri/usr/python/py-autoCookBook/test_site/simple_delay_server.py\", line 25, in <module>\n",
      "    server.serve_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 232, in serve_forever\n",
      "    ready = selector.select(poll_interval)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd test_site; python simple_delay_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From same site download `ch03-crawl_web.py` and search for references to `python` or `crocodile` <br>\n",
    "But better previous server or this call, since Jupyter executes only one bang at a time\n",
    "```\n",
    "$ python ch03-crawl_web.py http://localhost:8000/ -p python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ch03-crawl_web.py http://localhost:8000/ -p python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Components of crawl_web.py\n",
    "1. A loop that goes through all the found links, in the `main` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(base_url, to_search):\n",
    "    checked_links = set()\n",
    "    to_check = [base_url]\n",
    "    max_checks = 10\n",
    "\n",
    "    while to_check and max_checks:\n",
    "        link = to_check.pop()\n",
    "        links = process_link(link, text=to_search)\n",
    "        checked_links.add(link)\n",
    "        for link in links:\n",
    "            if link in checked_links:\n",
    "                continue\n",
    "            checked_links.add(link)\n",
    "            to_check.append(link)\n",
    "        max_check -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. download and parse links in `parse_link`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "import http\n",
    "def process_link(source_link, pat):\n",
    "    logging.info(f'exctracting links from {source_link}')\n",
    "    result = requests.get()\n",
    "    if result.status_code != http.client.ok:\n",
    "        logging.error(f'Failed retrieve {source_link}: {result}')\n",
    "        return []\n",
    "    if 'html' not in result.headers['Content-type']: #skip PDF\n",
    "        logging.info(f'Not HTML: {source_link}')\n",
    "        return []\n",
    "    page = BeautifulSoup(result.text,'html.parser')\n",
    "    search_text(source_link,page,pat)\n",
    "    parsed_source = urlparse(source_link) # divides URL to elements: http site path etc\n",
    "    return get_links(parsed_source,page)\n",
    "\n",
    "def search_text(source_link, page, pat):\n",
    "    '''print elements with text pattern'''\n",
    "    for el in page.find_all(text = re.compile(pat,flags=re.IGNORECASE) ) :\n",
    "        print(f'Link {source_link} ==> {el}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The `get_links` function retrieves all links on a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "def get_links(parsed_source,page):\n",
    "    '''retieve links on the page'''\n",
    "    links = []\n",
    "    for el in page.find_all('a'): # <a> elements\n",
    "        link = el.get('href')\n",
    "        if not link: continue\n",
    "        if link.startwith('#'): continue  # inside page\n",
    "        if link.startwith('mailto:'): continue  \n",
    "        if not link.startwith('http'):    # local link\n",
    "            netloc = parsed_source.netloc\n",
    "            scheme = parsed_source.scheme\n",
    "            path = urljoin(parsed_source.path, link)\n",
    "            link = f'{scheme}://{netloc}{path}'\n",
    "        if parsed_source.netloc not in link: # accept only same domain\n",
    "            continue\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subscribing to feeds\n",
    "`$ echo \"feedparser==5.2.1\" >> requirements.txt`  `use_2to3` is invalid\n",
    "Since this __[dirty fix](https://pypi.org/project/feedparser/5.2.1/)__ is ugly, I'll skip this section for RSS\n",
    "\n",
    "### Accessing web APIs\n",
    "RESTful API using __[JSON](https://www.json.org/)__  -- `requests` has native support<br>\n",
    "__[RESTful](https://codewords.recurse.com/issues/five/what-restful-actually-means)__ uses GET POST DELETE etc\n",
    "\n",
    "We will use __[https://jsonplaceholder.typicode.com](https://jsonplaceholder.typicode.com)__ -- It simulates a common case with posts, comments, and other common resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 10,\n",
       " 'id': 100,\n",
       " 'title': 'at nam consequatur ea labore ea harum',\n",
       " 'body': 'cupiditate quo est a modi nesciunt soluta\\nipsa voluptas error itaque dicta in\\nautem qui minus magnam et distinctio eum\\naccusamus ratione error aut'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "result = requests.get('https://jsonplaceholder.typicode.com/posts')  # <Response [200]>\n",
    "result.json() # 100 posts\n",
    "result.json()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = {'userId' : 10, 'title' : 'a title', 'body' : 'some stuff'}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9c68485fc2ca22e89981470a9b901ee6a2b3827ae489e275685bf84f6ed5b5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
